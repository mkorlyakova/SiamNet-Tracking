# SiamNet-Tracking
Simple SiamNet Tracking (pretrained SSD-MobileNet Detector + SiamNet Training(Train  SiamNet ( CIFAR).jpynb) and Test (Detector +  SiamNet.jpynb))

Модель GoTurn ттрекинга : слежение через обученную нейросеть сеть по OpenCV  имеет низкую производительность . Кроме того , строит слежение за одним объектом. в задаче MOT (Multy Object Tracking) нужно пререзапускать с каждым объектом отдельно или  держать n трекеров (память).

Проверяю вариант трекинга MOT с парой: 
 - Детектор SSD-MobileNet в каждом кадре
 - матчинг объектов через обученную нейросеть сиамского типа
 
## Этапы:
### 1.  Обучение siamNet (Train  SiamNet ( CIFAR).jpynb):
Обучаем на очень простой форме сверточную части сиамской сети, т.к. нужно просто обеспечить решениен задачи обработки кадров):
  - обучаем на CIFAR100
  - конструкция сверточной части сиама построена как результат сеточного поиска с целью получить минимальную ошибку классификации и используем только сверточную часть модели : Input((32,32,3))-Conv2D(32,(5,5),'relu')-Conv2D(32,(3,3),'relu')-Maxpooling()-Conv2D(32,(3,3),'relu')-Conv2D(32,(5,5),'relu')-Maxpooling()-Flatten
  - аугментация данных для обучения: Dataset for SiamNet Train:

    - 1th Imagei - cifar100, 2th Imagei + augmentation(cifar100) -> Label = 1
    - 1th Imagei - cifar100, 2th Imagej - cifar100 -> Label = 0

    - 1th Image - X1
    - 2th Image - X2
    - Label - L 
  - сохраняем веса Сиамской сети в siam_net_weight.h5  
  - по итогам обучения создана функция для генерации модели сиамской сети: new_model = SiamNet_create(siam_weigth ='/content/SiamNet-Tracking/siam_net_weight.h5' ), в которую передаем веса сети siam_net_weight.h5
  
 ### 2. Строим трекеры (Detector +  SiamNet.jpynb):
    Вариант_2:
    - запускаем Детектор по tensorflow_hub (https://www.tensorflow.org/hub/tutorials/tf2_object_detection)
    - MobileNet-SSD
    - Создаю Siamnet model_ = SiamNet_create(siam_weigth ='/content/SiamNet-Tracking/siam_net_weight.h5' )
    - Читаем примеры из папки ./Image/
    - Запускаем слежение:
      1) читаем нулевой кадр и заполняем List_coord список координат объектов, где для каждого пишем : [[ymin, xmin, ymax, xmax], номер кадра = 0] 
      2) одновременно пишем список изображений от детектора imlist2 - и нормализуем его по уровняям [0,1] и размеру (32х32х3)
      3) для следующих кадров повторяем:
        3.1) List_coord_old =List_coord imlist1=imlist2
        3.2) заполняем List_coord список координат объектов, где для каждого пишем : [[ymin, xmin, ymax, xmax], номер кадра = m]
        3.3) одновременно пишем список изображений от детектора imlist2 - и нормализуем его по уровняям [0,1] и размеру (32х32х3)
        3.4) делаем I_1,I_2 сетку примеров - все со всеми
        3.5) подаем примеры в сиамскую сеть predict()
        3.6) отсеиваемм примеры с порогом распознавания менее porog
        3.7) добавляем хорошие треки к списку найденых под кадров
      - визуализапция и оценка производительности
      
      Вариант_1:
      - запуск GoTurn трекинга из OpenCV (https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/)
      - запуск детектора для инициализации списка доступных для работы областей
      - для каждой области:
        - инициализация трекера по очередной области
        - слежениен
        - визуализация
      - подсчет статистики

### 3. Анализ результатов

Трекеры отработали MOT

Результат Варианта_2 нуждается в доработке в направлении объединения вероятных реализаций одного объекта (взять из матчинга базоваых трекеров)
Результат Варианта_1 быстро теряет концентрацию на объекте :(

Резюме:

- SiamNet+(Detector в каждом кадре) быстрее в 20 раз(не очень понятно пока в чем причина) чем OpenCV GoTurn (однократный Детектор + слежение по SiamNet).
- средняя длина трека для SiamNet+Detector без включения вероятных путей и анализа разрывов 2.7-2.0 - (нужно добавить объединение дублей и вероятных треков)
- OpenCV GoTurn (однократный Детектор + слежение по SiamNet) за 3-4 кадра потерял качество по IoU
      
       - 
